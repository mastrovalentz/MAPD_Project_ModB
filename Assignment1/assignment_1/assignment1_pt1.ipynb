{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "import os\n",
    "import json\n",
    "from operator import itemgetter\n",
    "from operator import add\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and repartition\n",
    "filename = os.path.join('data', 'papers_in_json_singleline', '*.json')\n",
    "lines = db.read_text(filename)\n",
    "js = lines.map(json.loads).repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge all the body texts in one for each file\n",
    "def merge(record):\n",
    "    text=''\n",
    "    for rec in record:\n",
    "        text+=rec['text']\n",
    "    return text\n",
    "\n",
    "texts = js.pluck(\"body_text\").map(merge)\n",
    "texts.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function for text cleaning\n",
    "#we remove punctuation, numbers and stopwords \n",
    "#stopwords are taken from a library but also definied by us\n",
    "#after this files are lists of words\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "no_words={\"i\", \"as\", \"or\", \"it\", \"et\", \"also\", \"may\"}\n",
    "\n",
    "def clean_func(text):\n",
    "\n",
    "    def merge_text(text,stop_words):\n",
    "        new_sentence =''\n",
    "        for w in text:\n",
    "            if w.lower() not in stop_words and w.isalpha(): \n",
    "                new_sentence += w \n",
    "                new_sentence += \" \"\n",
    "        return new_sentence\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    result = word_tokenize(text)\n",
    "    result = merge_text(result,stop_words.union(no_words))\n",
    "    result = tokenizer.tokenize(result)\n",
    "    return result\n",
    "\n",
    "text_clean = texts.map(clean_func)\n",
    "text_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we transform each list in a list of dictionaries \n",
    "#where the unique words and their frequency is stored\n",
    "\n",
    "def count_words(text):\n",
    "    counts = dict(zip(Counter(text).keys(), Counter(text).values()))\n",
    "    wList = [{\"word\":x , \"counts\": y} for x,y in counts.items()]\n",
    "    return wList\n",
    "\n",
    "words = text_clean.map(count_words)\n",
    "words.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# at last we sum over all file using the foldby method\n",
    "# we pass the bag to the foldby after flattening it \n",
    "#the methos accesse the dictiornarios gropyng them with the value associated to the key word\n",
    "# and the sum the counts of each word\n",
    "\n",
    "def incr_amount(tot, x):\n",
    "    return tot+x['counts']\n",
    "\n",
    "\n",
    "total_counts = words.flatten().foldby('word', binop=incr_amount, \n",
    "                   initial=0, \n",
    "                   combine=add, \n",
    "                   combine_initial=0).compute()\n",
    "\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# at last we order the obtained list\n",
    "\n",
    "total_counts_ordered = sorted(total_counts, key=itemgetter(1), reverse=True)\n",
    "total_counts_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timing of the operation\n",
    "\n",
    "start = time.time()\n",
    "word_count = (lines.map(json.loads).repartition(10).pluck('body_text')\n",
    "                   .map(merge).map(clean_func)\n",
    "                   .map(count_words).flatten()\n",
    "                   .foldby('word', binop=incr_amount, \n",
    "                        initial=0, combine=add, \n",
    "                        combine_initial=0).compute())\n",
    "word_sorted = sorted(word_count, key=itemgetter(1), reverse=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time: \", end - start)\n",
    "print(word_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=2)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "word_count = (lines.map(json.loads).repartition(10).pluck('body_text')\n",
    "                   .map(merge).map(clean_func)\n",
    "                   .map(count_words).flatten()\n",
    "                   .foldby('word', binop=incr_amount, \n",
    "                        initial=0, combine=add, \n",
    "                        combine_initial=0).compute())\n",
    "word_sorted = sorted(word_count, key=itemgetter(1), reverse=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time: \", end - start)\n",
    "print(word_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(workers=4, partitions=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    start = time.time() #strat taking time\n",
    "    word_count = (lines.map(json.loads).repartition(partitions).pluck('body_text')\n",
    "                       .map(merge).map(clean_func)\n",
    "                       .map(count_words).flatten()\n",
    "                       .foldby('word', binop=incr_amount, \n",
    "                            initial=0, combine=add, \n",
    "                            combine_initial=0).compute())\n",
    "    word_sorted = sorted(word_count, key=itemgetter(1), reverse=True)\n",
    "    end = time.time()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return end-start\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nw in range (3,5):\n",
    "#    print(\"nw \", nw, \" time \", get_time(nw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works = [1,2,3,4,5,6,7,8]\n",
    "#parts = [1,2,5,10,50, 100, 150]\n",
    "works = [8]\n",
    "parts = [8]\n",
    "for w in works:\n",
    "    for p in parts:\n",
    "        print(\"nw: \", w, \" Partitions \", p, \" time: \", get_time(w,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time(12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time(24, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.palettes as palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = [word_sorted[i][1] for i in range(0,len(word_sorted))]\n",
    "bars = [word_sorted[i][0] for i in range(0,len(word_sorted))]\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "index = 30\n",
    "# Create bars\n",
    "ax.barh(y_pos[:index], height[:index],color=palette.inferno(index))\n",
    "plt.yticks(y_pos[:index], bars[:index])\n",
    "ax.grid(True, which=\"both\", ls=\"-\",color='0.93')\n",
    "ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'papers_in_json_singleline', '*.json')\n",
    "lines = db.read_text(filename)\n",
    "js = lines.map(json.loads).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(n_workers=4)\n",
    "client= Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "js.pluck('metadata').pluck('authors').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = js.pluck('metadata').pluck('authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flatten(record):\n",
    "    uni=loc=lab=''\n",
    "    if 'institution' in record['affiliation'].keys():\n",
    "        uni=record['affiliation']['institution']\n",
    "    else: uni=\"Unknown\"\n",
    "    \n",
    "    if 'laboratory' in record['affiliation'].keys():\n",
    "        lab=record['affiliation']['laboratory']\n",
    "    else: lab=\"Unknown\"\n",
    "    \n",
    "    if 'location' in record['affiliation'].keys():\n",
    "        if 'country' in record['affiliation']['location'].keys():\n",
    "            loc=record['affiliation']['location']['country']\n",
    "        else: loc=\"Unknown\"\n",
    "    else: loc=\"Unknown\"\n",
    "    \n",
    "    if uni=='': uni=\"Unknown\"\n",
    "    if lab=='': lab=\"Unknown\"\n",
    "    if loc=='': loc=\"Unknown\"\n",
    "    \n",
    "    return {\n",
    "        'name':       record['first'],\n",
    "        'surname':    record['last'], \n",
    "        'University': uni,\n",
    "        'Laboratory': lab,\n",
    "        'Country':    loc,\n",
    "    }\n",
    "\n",
    "authors.flatten().map(flatten).take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_df = authors.flatten().map(flatten).to_dataframe()\n",
    "auth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univs = auth_df.University.value_counts().nlargest(10).compute()\n",
    "univs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labos = auth_df.Laboratory.value_counts().nlargest(10).compute()\n",
    "labos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = auth_df.Country.value_counts().nlargest(10).compute()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_univs(workers=4, partitions=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    start = time.time() #strat taking time\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    univs = auth_df.University.value_counts().nlargest(10).compute()\n",
    "    end = time.time()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return end-start\n",
    "\n",
    "def get_time_countries(workers=4, partitions=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    start = time.time() #strat taking time\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    countries = auth_df.Country.value_counts().nlargest(10).compute()\n",
    "    end = time.time()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return end-start\n",
    "\n",
    "def get_N_univs(workers=4, partitions=10, N=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    univs = auth_df.University.value_counts().nlargest(N).compute()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return univs\n",
    "def get_N_countries(workers=4, partitions=10, N=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    countries = auth_df.Country.value_counts().nlargest(N).compute()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return countries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_univs(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_countries(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_N_univs(10,10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_N_countries(10,10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37987</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>12</li>\n",
       "  <li><b>Memory: </b>16.80 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37987' processes=4 threads=12, memory=16.80 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster=LocalCluster(n_workers=4)\n",
    "client= Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'papers_in_json_singleline', '*.json')\n",
    "lines = db.read_text(filename)\n",
    "js = lines.map(json.loads).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metas = js.pluck([\"paper_id\", \"metadata\"])\n",
    "m=metas.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PfSWIB, a potential chromatin regulator for var gene regulation and parasite development in Plasmodium falciparum\n"
     ]
    }
   ],
   "source": [
    "print (m[1][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get_input_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dbccc21d5246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#ft = fasttext.load_model('/home/alessandro/Downloads/wiki-news-300d-1M.vec')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#ft.get_dimension()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'home/alessandro/Download/cc.en.300.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/MAPD/lib/python3.6/site-packages/fasttext/util/util.py\u001b[0m in \u001b[0;36mreduce_model\u001b[0;34m(ft_model, target_dim)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \"\"\"\n\u001b[1;32m    120\u001b[0m     inp_reduced, proj = _reduce_matrix(\n\u001b[0;32m--> 121\u001b[0;31m         ft_model.get_input_matrix(), target_dim, None)\n\u001b[0m\u001b[1;32m    122\u001b[0m     out_reduced, _ = _reduce_matrix(\n\u001b[1;32m    123\u001b[0m         ft_model.get_output_matrix(), target_dim, proj)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get_input_matrix'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('/home/alessandro/Downloads/cc.en.300.bin')\n",
    "ft.get_dimension() \n",
    "fasttext.util.reduce_model(ft, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "#model = load_vectors('/home/alessandro/Downloads/wiki-news-300d-1M.vec')\n",
    "#to get the embedding of word ’hello’:\n",
    "#model['hello']\n",
    "\n",
    "model = fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = (m[1][\"title\"])\n",
    "text_split = text.split()\n",
    "\n",
    "text_embedded = []\n",
    "for t in text_split:\n",
    "    try:\n",
    "        text_embedded.append(model[t])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#text_embedded =[ model[t] for t in text_split]\n",
    "text_embedded\n",
    "#text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    t = np.array(model['PfSWIB'])\n",
    "except:\n",
    "    t= np.zeros(1)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd (text):\n",
    "    text_split = text.split()\n",
    "    text_embedded = []\n",
    "    for t in text_split:\n",
    "        try:\n",
    "            text_embedded.append(model[t])\n",
    "        except:\n",
    "            pass\n",
    "    return text_embedded\n",
    "\n",
    "def reco_emb(reco):\n",
    "    print(reco)\n",
    "    return {\n",
    "        \"paper_id\": reco['paper_id'],\n",
    "        \"title\": embedd(reco['title'])\n",
    "    }\n",
    "\n",
    "def flatten(reco):\n",
    "    text= reco[1]['title']\n",
    "    #emb_text=embedd(text)\n",
    "    print(text)\n",
    "    return {\n",
    "        \"paper_id\": reco[0],\n",
    "        \"title\": reco[1]['title']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = db.from_sequence(metas.map(flatten).compute())\n",
    "titles.take(3)\n",
    "\n",
    "\n",
    "#titles = metas.map(flatten_embedding)\n",
    "#titles.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = titles.pluck('title').take(1)\n",
    "#reco_emb(t[0])\n",
    "t=t[0]\n",
    "t\n",
    "#ids = titles.pluck('paper_id').take(3)\n",
    "#et = [embedd(x) for x in t]\n",
    "\n",
    "model['hello']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = t.map(reco_emb).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = metas.map(flatten).to_dataframe().compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "#c = list(itertools.product(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = db.from_sequence(list(itertools.product(small_titl, small_titl))).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.take(2, npartitions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.product(small_titl, small_titl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
