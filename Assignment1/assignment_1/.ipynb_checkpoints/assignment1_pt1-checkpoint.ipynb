{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed\n",
    "\n",
    "import dask.bag as db\n",
    "import os\n",
    "import json\n",
    "from operator import itemgetter\n",
    "from operator import add\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from operator import iconcat\n",
    "import functools\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and repartition\n",
    "filename = os.path.join('data', 'papers_in_json_singleline', '*.json')\n",
    "lines = db.read_text(filename)\n",
    "js = lines.map(json.loads).repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge all the body texts in one for each file\n",
    "def merge(record):\n",
    "    text=''\n",
    "    for rec in record:\n",
    "        text+=rec['text']\n",
    "    return text\n",
    "\n",
    "texts = js.pluck(\"body_text\").map(merge)\n",
    "texts.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function for text cleaning\n",
    "#we remove punctuation, numbers and stopwords \n",
    "#stopwords are taken from a library but also definied by us\n",
    "#after this files are lists of words\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "no_words={\"i\", \"as\", \"or\", \"it\", \"et\", \"also\", \"may\"}\n",
    "\n",
    "def clean_func(text):\n",
    "\n",
    "    def merge_text(text,stop_words):\n",
    "        new_sentence =''\n",
    "        for w in text:\n",
    "            if w.lower() not in stop_words and w.isalpha(): \n",
    "                new_sentence += w \n",
    "                new_sentence += \" \"\n",
    "        return new_sentence\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    result = word_tokenize(text)\n",
    "    result = merge_text(result,stop_words.union(no_words))\n",
    "    result = tokenizer.tokenize(result)\n",
    "    return result\n",
    "\n",
    "text_clean = texts.map(clean_func)\n",
    "text_clean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we transform each list in a list of dictionaries \n",
    "#where the unique words and their frequency is stored\n",
    "\n",
    "def count_words(text):\n",
    "    counts = dict(zip(Counter(text).keys(), Counter(text).values()))\n",
    "    wList = [{\"word\":x , \"counts\": y} for x,y in counts.items()]\n",
    "    return wList\n",
    "\n",
    "words = text_clean.map(count_words)\n",
    "words.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# at last we sum over all file using the foldby method\n",
    "# we pass the bag to the foldby after flattening it \n",
    "#the methos accesse the dictiornarios gropyng them with the value associated to the key word\n",
    "# and the sum the counts of each word\n",
    "\n",
    "def incr_amount(tot, x):\n",
    "    return tot+x['counts']\n",
    "\n",
    "\n",
    "total_counts = words.flatten().foldby('word', binop=incr_amount, \n",
    "                   initial=0, \n",
    "                   combine=add, \n",
    "                   combine_initial=0).compute()\n",
    "\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# at last we order the obtained list\n",
    "\n",
    "total_counts_ordered = sorted(total_counts, key=itemgetter(1), reverse=True)\n",
    "total_counts_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timing of the operation\n",
    "\n",
    "start = time.time()\n",
    "word_count = (lines.map(json.loads).repartition(10).pluck('body_text')\n",
    "                   .map(merge).map(clean_func)\n",
    "                   .map(count_words).flatten()\n",
    "                   .foldby('word', binop=incr_amount, \n",
    "                        initial=0, combine=add, \n",
    "                        combine_initial=0).compute())\n",
    "word_sorted = sorted(word_count, key=itemgetter(1), reverse=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time: \", end - start)\n",
    "print(word_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=2)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "word_count = (lines.map(json.loads).repartition(10).pluck('body_text')\n",
    "                   .map(merge).map(clean_func)\n",
    "                   .map(count_words).flatten()\n",
    "                   .foldby('word', binop=incr_amount, \n",
    "                        initial=0, combine=add, \n",
    "                        combine_initial=0).compute())\n",
    "word_sorted = sorted(word_count, key=itemgetter(1), reverse=True)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time: \", end - start)\n",
    "print(word_sorted[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(workers=4, partitions=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    start = time.time() #strat taking time\n",
    "    word_count = (lines.map(json.loads).repartition(partitions).pluck('body_text')\n",
    "                       .map(merge).map(clean_func)\n",
    "                       .map(count_words).flatten()\n",
    "                       .foldby('word', binop=incr_amount, \n",
    "                            initial=0, combine=add, \n",
    "                            combine_initial=0).compute())\n",
    "    word_sorted = sorted(word_count, key=itemgetter(1), reverse=True)\n",
    "    end = time.time()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return end-start\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for nw in range (3,5):\n",
    "#    print(\"nw \", nw, \" time \", get_time(nw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works = [1,2,3,4,5,6,7,8]\n",
    "#parts = [1,2,5,10,50, 100, 150]\n",
    "works = [8]\n",
    "parts = [8]\n",
    "for w in works:\n",
    "    for p in parts:\n",
    "        print(\"nw: \", w, \" Partitions \", p, \" time: \", get_time(w,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time(12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time(24, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.palettes as palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = [word_sorted[i][1] for i in range(0,len(word_sorted))]\n",
    "bars = [word_sorted[i][0] for i in range(0,len(word_sorted))]\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "index = 30\n",
    "# Create bars\n",
    "ax.barh(y_pos[:index], height[:index],color=palette.inferno(index))\n",
    "plt.yticks(y_pos[:index], bars[:index])\n",
    "ax.grid(True, which=\"both\", ls=\"-\",color='0.93')\n",
    "ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'papers_in_json_singleline', '*.json')\n",
    "lines = db.read_text(filename)\n",
    "js = lines.map(json.loads).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(n_workers=4)\n",
    "client= Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "js.pluck('metadata').pluck('authors').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = js.pluck('metadata').pluck('authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flatten(record):\n",
    "    uni=loc=lab=''\n",
    "    if 'institution' in record['affiliation'].keys():\n",
    "        uni=record['affiliation']['institution']\n",
    "    else: uni=\"Unknown\"\n",
    "    \n",
    "    if 'laboratory' in record['affiliation'].keys():\n",
    "        lab=record['affiliation']['laboratory']\n",
    "    else: lab=\"Unknown\"\n",
    "    \n",
    "    if 'location' in record['affiliation'].keys():\n",
    "        if 'country' in record['affiliation']['location'].keys():\n",
    "            loc=record['affiliation']['location']['country']\n",
    "        else: loc=\"Unknown\"\n",
    "    else: loc=\"Unknown\"\n",
    "    \n",
    "    if uni=='': uni=\"Unknown\"\n",
    "    if lab=='': lab=\"Unknown\"\n",
    "    if loc=='': loc=\"Unknown\"\n",
    "    \n",
    "    return {\n",
    "        'name':       record['first'],\n",
    "        'surname':    record['last'], \n",
    "        'University': uni,\n",
    "        'Laboratory': lab,\n",
    "        'Country':    loc,\n",
    "    }\n",
    "\n",
    "authors.flatten().map(flatten).take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_df = authors.flatten().map(flatten).to_dataframe()\n",
    "auth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univs = auth_df.University.value_counts().nlargest(10).compute()\n",
    "univs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labos = auth_df.Laboratory.value_counts().nlargest(10).compute()\n",
    "labos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = auth_df.Country.value_counts().nlargest(10).compute()\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_univs(workers=4, partitions=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    start = time.time() #strat taking time\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    univs = auth_df.University.value_counts().nlargest(10).compute()\n",
    "    end = time.time()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return end-start\n",
    "\n",
    "def get_time_countries(workers=4, partitions=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    start = time.time() #strat taking time\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    countries = auth_df.Country.value_counts().nlargest(10).compute()\n",
    "    end = time.time()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return end-start\n",
    "\n",
    "def get_N_univs(workers=4, partitions=10, N=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    univs = auth_df.University.value_counts().nlargest(N).compute()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return univs\n",
    "def get_N_countries(workers=4, partitions=10, N=10):\n",
    "    myCluster = LocalCluster(n_workers=workers)\n",
    "    client = Client(myCluster) #make client\n",
    "    auth_df = (lines.map(json.loads).repartition(10)\n",
    "                    .pluck('metadata').pluck('authors')\n",
    "                    .flatten().map(flatten)\n",
    "                    .to_dataframe())\n",
    "    countries = auth_df.Country.value_counts().nlargest(N).compute()\n",
    "    client.close() #close client\n",
    "    myCluster.close() #close cluster\n",
    "    return countries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_univs(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_time_countries(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_N_univs(10,10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_N_countries(10,10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(n_workers=4)\n",
    "client= Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'papers_in_json_singleline', '*.json')\n",
    "lines = db.read_text(filename)\n",
    "js = lines.map(json.loads).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metas = js.pluck([\"paper_id\", \"metadata\"])\n",
    "m=metas.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (m[1][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('/home/alessandro/Downloads/cc.en.300.bin')\n",
    "ft.get_dimension() \n",
    "fasttext.util.reduce_model(ft, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "#model = load_vectors('/home/alessandro/Downloads/wiki-news-300d-1M.vec')\n",
    "#to get the embedding of word ’hello’:\n",
    "#model['hello']\n",
    "\n",
    "model = fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = (m[1][\"title\"])\n",
    "text_split = text.split()\n",
    "\n",
    "text_embedded = []\n",
    "for t in text_split:\n",
    "    try:\n",
    "        text_embedded.append(model[t])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#text_embedded =[ model[t] for t in text_split]\n",
    "text_embedded\n",
    "#text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    t = np.array(model['PfSWIB'])\n",
    "except:\n",
    "    t= np.zeros(1)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedd (text):\n",
    "    text_split = text.split()\n",
    "    text_embedded = []\n",
    "    for t in text_split:\n",
    "        try:\n",
    "            text_embedded.append(model[t])\n",
    "        except:\n",
    "            pass\n",
    "    return text_embedded\n",
    "\n",
    "def reco_emb(reco):\n",
    "    print(reco)\n",
    "    return {\n",
    "        \"paper_id\": reco['paper_id'],\n",
    "        \"title\": embedd(reco['title'])\n",
    "    }\n",
    "\n",
    "def flatten(reco):\n",
    "    text= reco[1]['title']\n",
    "    #emb_text=embedd(text)\n",
    "    print(text)\n",
    "    return {\n",
    "        \"paper_id\": reco[0],\n",
    "        \"title\": reco[1]['title']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titles = db.from_sequence(metas.map(flatten).compute())\n",
    "titles.take(3)\n",
    "\n",
    "\n",
    "#titles = metas.map(flatten_embedding)\n",
    "#titles.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = titles.pluck('title').take(1)\n",
    "#reco_emb(t[0])\n",
    "t=t[0]\n",
    "t\n",
    "#ids = titles.pluck('paper_id').take(3)\n",
    "#et = [embedd(x) for x in t]\n",
    "\n",
    "model['hello']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = t.map(reco_emb).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = metas.map(flatten).to_dataframe().compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#c = list(itertools.product(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = db.from_sequence(list(itertools.product(small_titl, small_titl))).repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.take(2, npartitions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(itertools.product(small_titl, small_titl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTE 3 FUNZIONANTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=LocalCluster(n_workers=8)\n",
    "client= Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'embedded_papers', '*.json')\n",
    "js = db.read_text(filename).map(json.loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_to_float(l):\n",
    "    return np.array([[float(n) for n in arr]for arr in l])\n",
    "\n",
    "def db_to_float(reco):\n",
    "    return {\n",
    "        \"paper_id\": reco['paper_id'],\n",
    "        \"title\": back_to_float(reco['title'])\n",
    "    }\n",
    "\n",
    "titles = db.from_sequence(js.map(db_to_float).compute())\n",
    "\n",
    "#il modell che usiamo converte ogni parola in un array di 300 numeri\n",
    "#quindi ogni titolo compost da n parole sarà ora un np array nx300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list= js.map(db_to_float).compute()\n",
    "\n",
    "def cos_sim(x,y):\n",
    "    l=min(x.shape[0], y.shape[0])\n",
    "    nx = np.array([np.linalg.norm(x1) for x1 in x])\n",
    "    ny = np.array([np.linalg.norm(y1) for y1 in y])\n",
    "    prod = np.sum(x[:l]*y[:l], axis=1)/(nx*ny)\n",
    "    return np.mean(prod)\n",
    "\n",
    "\n",
    "def make_sim_reco(reco1, reco2):\n",
    "    try:\n",
    "        c= cos_sim(reco1['title'], reco2['title'])\n",
    "    except:\n",
    "        c=0\n",
    "    return {\n",
    "        \"Paper 1:\": reco1['paper_id'],\n",
    "        \"Paper 2:\": reco2['paper_id'],\n",
    "        \"Similarity\": c\n",
    "    }\n",
    "\n",
    "def make_sim_bag(reco):\n",
    "    try:\n",
    "        c= cos_sim(reco[0]['title'], reco[1]['title'])\n",
    "    except:\n",
    "        c=0\n",
    "    return {\n",
    "        \"Paper 1:\": reco[0]['paper_id'],\n",
    "        \"Paper 2:\": reco[1]['paper_id'],\n",
    "        \"Similarity\": c\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sims=[]\n",
    "##funzia ma è lento\n",
    "##volentdo si può fare doppio ciclo\n",
    "#\n",
    "#for i in range(len(title_list)):\n",
    "#    #for j in range(10):#((i+1),len(title_list)): #avoid doubles and comparison with itself\n",
    "#    #nj = min(i+101, len(title_list))\n",
    "#    if i%10 ==0: print(i)\n",
    "#    batch= db.from_sequence([(title_list[i], title_list[j]) for j in range(i+1, len(title_list))]).repartition(100)\n",
    "#    sims.append(batch.map(make_sim_bag).compute())\n",
    "#    del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(title_list)\n",
    "nb=100\n",
    "\n",
    "#gira funziona ma è lentissimo\n",
    "\n",
    "\n",
    "sims=[]\n",
    "for i in range(int(l/nb)+1):\n",
    "    print(i)\n",
    "    for j in range(i, int(l/nb)+1):\n",
    "        #ll=[(i*nb+k,j*nb+l) for k in range(nb) for l in range(nb) ]\n",
    "        batch= db.from_sequence([(title_list[k], title_list[m]) for k in range(i*nb, min((i+1)*nb,l)) for m in range(j*nb, min((j+1)*nb+i,l)) ]).repartition(8)\n",
    "        sims.append(batch.map(make_sim_bag).compute())\n",
    "        del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sims= batch.map(make_sim_bag).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarities = functools.reduce(iconcat, sims, [])\n",
    "\n",
    "similarities[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = titles.map(make_sim_bag).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list= js.map(db_to_float).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_sim_reco(title_list[9], title_list[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import sys\n",
    "\n",
    "temp_res=[]\n",
    "results = []\n",
    "count=0\n",
    "for i in range(len(title_list)):\n",
    "    for j in range((i+1),len(title_list)): #avoid doubles and comparison with itself\n",
    "        count+=1\n",
    "        s = client.submit(make_sim_reco,title_list[i], title_list[j])\n",
    "        temp_res.append(s)\n",
    "        if count==1000:\n",
    "            res = client.gather(temp_res)\n",
    "            tem_res=[]\n",
    "            results.append(res)\n",
    "            count=0\n",
    "        \n",
    "res = client.gather(temp_res)\n",
    "results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles =db.from_sequence(t).repartition(100)\n",
    "grid = titles.map(make_pairs).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem = grid.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forse da distemare la metrica ma funzia\n",
    "\n",
    "def cos_sim(x,y):\n",
    "    l=min(x.shape[0], y.shape[0])\n",
    "    nx = np.array([np.linalg.norm(x1) for x1 in x])\n",
    "    ny = np.array([np.linalg.norm(y1) for y1 in y])\n",
    "    prod = np.sum(x[:l]*y[:l], axis=1)/(nx*ny)\n",
    "    return np.mean(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(t1,t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = elem[0].take(2)    \n",
    "\n",
    "def make_sim(reco):\n",
    "    try:\n",
    "        c= cos_sim(reco[0]['title'], reco[1]['title'])\n",
    "    except:\n",
    "        c=0\n",
    "    return {\n",
    "        \"Paper 1:\": reco[0]['paper_id'],\n",
    "        \"Paper 2:\": reco[1]['paper_id'],\n",
    "        \"Similarity\": c\n",
    "    }\n",
    "\n",
    "def make_sim_bag(reco):\n",
    "    return reco.map(make_sim).compute().re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities = grid.map(make_sim_bag).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t1.shape)\n",
    "print(t2.shape)\n",
    "n= min(t1.shape[0], t2.shape[0])\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "[np.dot(t1[i], t2[i]), for i in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2], [3,4]])\n",
    "b=np.array([[-1,-2], [0.5, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inner(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(a*b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2], [3,4]])\n",
    "a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(a*a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([np.linalg.norm(b) for b in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.array([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([2,3, 1,-1, 5])\n",
    "\n",
    "[(b,c) for b in a for c in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.argpartition(-a, 4)\n",
    "result_args = temp[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=3\n",
    "l=10\n",
    "for i in range(int(l/nb)+1):\n",
    "    for j in range(i, int(l/nb)+1):\n",
    "        #ll=[(i*nb+k,j*nb+l) for k in range(nb) for l in range(nb) ]\n",
    "        ll=[(k,m) for k in range(i*nb, min((i+1)*nb,l)) for m in range(j*nb, min((j+1)*nb+i,l)) ]\n",
    "        print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "l=200#len(title_list)\n",
    "nb=10\n",
    "\n",
    "#gira funziona ma è lentissimo\n",
    "\n",
    "\n",
    "sims=[]\n",
    "for i in range(int(l/nb)+1):\n",
    "    for j in range(i, int(l/nb)+1):\n",
    "        #ll=[(i*nb+k,j*nb+l) for k in range(nb) for l in range(nb) ]\n",
    "        batch= db.from_sequence([(title_list[k], title_list[m]) for k in range(i*nb, min((i+1)*nb,l)) for m in range(j*nb, min((j+1)*nb+i,l)) ]).repartition(8)\n",
    "        sims.append(batch.map(make_sim_bag).compute())\n",
    "        del batch\n",
    "        \n",
    "sims_par = functools.reduce(iconcat, sims, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l=len(title_list)\n",
    "\n",
    "sims_ser=[]\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        sim = make_sim_reco(title_list[i], title_list[j])\n",
    "        sims_ser.append(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3 scritta un pelo bene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import delayed\n",
    "\n",
    "import dask.bag as db\n",
    "import os\n",
    "import json\n",
    "from operator import itemgetter\n",
    "from operator import add\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from operator import iconcat\n",
    "import functools\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary provided is too heavy to be used in jupyter, as well as the other model present at \n",
    "\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "In order to embedd the titles of the papers we followed these steps: both the model and dictioanry where too heavy to be uploaded through Jupyter so we used a Python script, visible here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat FastTextModel.py #da commentare un po di più"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded a pretrained module instead of a dictionary in order to be able to load it with the gensim package which is more efficient in terms of resources ( i think)\n",
    "In the script the following stepos were followed:\n",
    "\n",
    "    - a dask client and cluster is created\n",
    "    - the papers are retrived from the json files using a dask.bag\n",
    "    - the bag is flattened to a list contataining only title and id\n",
    "    - the model is loaded and the titles are embedded with it\n",
    "    - the model returns an array of 300 numbers fro each word so the outout is a dictionary with a value represented by a list of arrays\n",
    "    - in ordere to store the files (this procedure is too heavy for normal pc) the arrays have to be transformed to string lists\n",
    "    - the files are saved in json format\n",
    "    -clienbt and cluster are closed\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we can now load the embedded files in jupyter always using the dask bag inteface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:42305</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>16</li>\n",
       "  <li><b>Memory: </b>16.80 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:42305' processes=8 threads=16, memory=16.80 GB>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make client\n",
    "cluster=LocalCluster(n_workers=8)\n",
    "client= Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load files\n",
    "filename = os.path.join('data', 'embedded_papers', '*.json')\n",
    "js = db.read_text(filename).map(json.loads)\n",
    "js.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parte bonus\n",
    "\n",
    "The string format which we used to save the embedded files is not suitable for mathematical computation so we mapped the bag to itself but reverting the strings lissts to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_to_float(l):\n",
    "    return np.array([[float(n) for n in arr]for arr in l])\n",
    "\n",
    "def db_to_float(reco):\n",
    "    return {\n",
    "        \"paper_id\": reco['paper_id'],\n",
    "        \"title\": back_to_float(reco['title'])\n",
    "    }\n",
    "\n",
    "titles = db.from_sequence(js.map(db_to_float).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'paper_id': '000a0fc8bbef80410199e690191dc3076a290117',\n",
       "  'title': array([[-4.4659150e-02,  1.5186340e-02, -3.3508370e-02, ...,\n",
       "           1.7950660e-02, -9.7958100e-03, -2.5739150e-02],\n",
       "         [ 8.7643050e-02, -4.9590126e-01, -4.9854990e-02, ...,\n",
       "           5.3485096e-01, -5.5810360e-02, -1.6598430e-02],\n",
       "         [-2.8673020e-02, -2.5425730e-02, -5.1420850e-02, ...,\n",
       "           5.7633900e-02, -4.3897000e-04,  2.2708240e-02],\n",
       "         ...,\n",
       "         [-1.4047040e-02, -2.5217462e-01,  7.1501930e-02, ...,\n",
       "           1.3704422e-01,  4.5107400e-03,  3.2923460e-02],\n",
       "         [ 3.6612860e-02, -5.9771530e-02, -1.7483340e-02, ...,\n",
       "           1.0073350e-01,  2.0367010e-02,  2.8794700e-03],\n",
       "         [ 5.6244680e-02, -1.6394550e-02,  5.0384400e-03, ...,\n",
       "           3.6305250e-02, -5.7916600e-03, -9.9020800e-03]])},)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#magari va cambiata la metrica ma cos e il resto funziona\n",
    "\n",
    "def cos_sim(x,y):\n",
    "    l=min(x.shape[0], y.shape[0])\n",
    "    nx = np.array([np.linalg.norm(x1) for x1 in x])\n",
    "    ny = np.array([np.linalg.norm(y1) for y1 in y])\n",
    "    prod = np.sum(x[:l]*y[:l], axis=1)/(nx*ny)\n",
    "    return np.mean(prod)\n",
    "\n",
    "\n",
    "def make_sim_reco(reco1, reco2):\n",
    "    try:\n",
    "        c= cos_sim(reco1['title'], reco2['title'])\n",
    "    except:\n",
    "        c=0\n",
    "    return {\n",
    "        \"Paper 1:\": reco1['paper_id'],\n",
    "        \"Paper 2:\": reco2['paper_id'],\n",
    "        \"Similarity\": c\n",
    "    }\n",
    "\n",
    "def make_sim_bag(reco):\n",
    "    try:\n",
    "        c= cos_sim(reco[0]['title'], reco[1]['title'])\n",
    "    except:\n",
    "        c=0\n",
    "    return {\n",
    "        \"Paper 1:\": reco[0]['paper_id'],\n",
    "        \"Paper 2:\": reco[1]['paper_id'],\n",
    "        \"Similarity\": c\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity in our cases is computed by calculating the cosine of the angle from the title vectors up to the lowest number of words of the 2 (e.g the first 2 titles have 15 and 29 words respectively so we compute the cosine of the first 15 words) and mean over it, this procedure is then embedded in a function in order to be mapped.\n",
    "\n",
    "In order to map this word over a dask.bag we need to create a grid over a list and map it, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Paper 1:': '000a0fc8bbef80410199e690191dc3076a290117',\n",
       " 'Paper 2:': '000a0fc8bbef80410199e690191dc3076a290117',\n",
       " 'Similarity': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list= js.map(db_to_float).compute()\n",
    "batch= (db.from_sequence([(title_list[i], title_list[j]) for i in range(10) for j in range(10) ])\n",
    "          .repartition(8).map(make_sim_bag).compute())\n",
    "        \n",
    "batch[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of this approach is that for a small grid it is really fast but going to bigger grid it scales with $N^2$ over the memory consumption, for this reason is not possible to apply the above code to the full dataset, insted it is necessary to run it in batches. thiS procedure is very slow and it also requires the memory to be flushed by hand, therefore the required time is very high.\n",
    "\n",
    "As we can see from the cells below a serial approach is way faster and is also less lighter is terms of resources needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 3.08 s, total: 55.9 s\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "l=200#len(title_list)\n",
    "nb=10\n",
    "\n",
    "#gira funziona ma è lentissimo\n",
    "\n",
    "\n",
    "sims=[]\n",
    "for i in range(int(l/nb)+1):\n",
    "    for j in range(i, int(l/nb)+1):\n",
    "        #ll=[(i*nb+k,j*nb+l) for k in range(nb) for l in range(nb) ]\n",
    "        batch= db.from_sequence([(title_list[k], title_list[m]) for k in range(i*nb, min((i+1)*nb,l)) for m in range(j*nb, min((j+1)*nb+i,l)) ]).repartition(8)\n",
    "        sims.append(batch.map(make_sim_bag).compute())\n",
    "        del batch\n",
    "        \n",
    "sims_par = functools.reduce(iconcat, sims, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alessandro/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/alessandro/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.31 s, sys: 1.99 s, total: 9.3 s\n",
      "Wall time: 6.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "l=200#len(title_list)\n",
    "\n",
    "sims_ser=[]\n",
    "for i in range(l):\n",
    "    for j in range(l):\n",
    "        sim = make_sim_reco(title_list[i], title_list[j])\n",
    "        sims_ser.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
